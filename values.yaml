app: gemma-3-27b-it

modelName: google/gemma-3-27b-it
replicaCount: 1

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu.product
          operator: In
          values:
          # Add one of the following options:
          # - NVIDIA-H100-80GB-HBM3
          # - NVIDIA-H100-80GB-HBM3-MIG-1g.10gb
          # - NVIDIA-H100-80GB-HBM3-MIG-1g.20gb
          # - NVIDIA-H100-80GB-HBM3-MIG-1g.40gb
          - NVIDIA-L40S

tolerations: {}

image:
  repository: dekaregistry.cloudeka.id/cloudeka-system/vllm-openai
  tag: v0.10.2
  pullPolicy: Always

pvc:
  enabled: true
  name: ""
  storageClassName: storage-nvme-c1
  size: 250Gi

service:
  enabled: yes
  name: ""
  type: LoadBalancer
  annotations:
    metallb.universe.tf/address-pool: address-pool
  ports:
    - name: http-vllm
      port: 80
      targetPort: 8000
      protocol: TCP
  sessionAffinity: None

resources:
  limits:
    cpu: "16"
    memory: "32Gi"
    nvidia.com/gpu: "4"
  requests:
    cpu: "8"
    memory: "8Gi"
    nvidia.com/gpu: "4"

# Probe configurations for vLLM server
probes:
  liveness:
    path: /health
    port: 8000
    initialDelaySeconds: 30
    periodSeconds: 30
    failureThreshold: 40
  readiness:
    path: /health
    port: 8000
    initialDelaySeconds: 30
    periodSeconds: 30
    failureThreshold: 40

# Additional arguments passed to vllm serve
# Example:
# extraArgs:
#   - "--gpu-memory-utilization"
#   - "0.95"
#   - "--tensor-parallel-size"
#   - "8"
#   - "--enforce-eager"
extraArgs:
  - "--gpu-memory-utilization"
  - "0.9"
  - "--tensor-parallel-size"
  - "4"
  - "--max-num-batched-tokens"
  - "65536"
  # - "--data-parallel-size"
  # - "8"
  # - "--served-model-name"
  # - "qwen/qwen-25-72b-instruct"
  # - "--max-model-len"
  # - "131072"
  # - "--api-key"
  # - ""
  - "--enable-auto-tool-choice"
  - "--tool-call-parser"
  - "hermes"
  # - "--chat-template"
  # - "examples/tool_chat_template_llama4_pythonic.jinja"
  # - "--trust-remote-code"
  # - "--enforce-eager"
  # - "--enable-expert-parallel"

# Additional environment variables (optional)
# Example:
# env:
#   - name: HUGGING_FACE_HUB_TOKEN
#     value: <HUGGING_FACE_HUB_TOKEN>
env: {}